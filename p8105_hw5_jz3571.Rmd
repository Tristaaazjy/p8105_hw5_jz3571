---
title: "p8105_hw5_jz3571"
author: "Junyan Zhu"
date: "2022-11-14"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(tidyverse)
library(patchwork)
library(rvest)
```

## Problem 1

```{r, message=FALSE}
longstudy_df = 
  tibble(file_name = list.files(path =  "./data/P1") ) %>% 
  mutate(
    path = str_c("./data/P1/", file_name),
    data = map(.x = path, ~read_csv(.x))
  ) %>% 
  separate(file_name, into = c("arm", "id"), sep = "_") %>% 
  mutate(id = gsub(".csv", "", id),
         arm = recode(arm, con = "control", exp = 'experiment')) %>% 
  select(-path) %>% 
  unnest(data) %>% 
  pivot_longer(week_1: week_8,
               names_to = "week",
               values_to = "observation",
               names_prefix = "week_")
```

```{r}
longstudy_df 
```

```{r}
longstudy_df %>% 
  ggplot(aes(x = week, y = observation)) +
  geom_path(aes(color = arm,group = as.factor(id))) +
  labs(
    title = "Observations on Each Subject over Time",
    x = "Week",
    y = "Observation",
    caption = "Data from a longitudinal study") +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5))
  
```



The experiment arm has a higher observation data compared to control arm. The control and experiment arms had similar observations for the first several weeks. However, as time goes, the experiment arms have a increasing trend while the control arms have a fluctuation trend.


## Problem 2

#### Describe the raw data

```{r}
homicide_df = read_csv("./data/homicide-data.csv")
```

The raw data from Washington Post homicide data has 52179 rows and 12 columns. It recorded 52179 homicide cases with 12 variables, including uid, reported date, victim's last name, victim's first name, victim's race, age, sex, city, state, the location of the killing in latitude and longitude and also disposition.

#### Create city_state variable and summarize total and unresolved cases within cities

```{r}
homicide_city = homicide_df %>% 
  mutate(city_state = str_c(city, state, sep = "_"))

homicide_tidy = homicide_city %>% 
  group_by(city_state) %>% 
  summarise(total = n(), unresolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")) %>% 
  filter(city_state != "Tulsa_AL")

knitr::kable(homicide_tidy)

```

#### Prop.test for Baltimore_MD

```{r}
homicide_bal = homicide_tidy %>% 
  filter(city_state == "Baltimore_MD")

prop.test(homicide_bal$unresolved, homicide_bal$total) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable()
```

#### Prop.test for each city

```{r}
prop_test = function(x, y){
  
  test = prop.test(x,y) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high)
  
  test
  
}

prop_city =
  map2(.x = homicide_tidy$unresolved, .y = homicide_tidy$total,
       ~prop_test(.x, .y)) %>% 
  bind_rows() %>% 
  mutate(city_state = homicide_tidy$city_state) %>% 
  select(city_state, everything())

knitr::kable(prop_city)
```

#### Plot of estimates and CIs for each city

```{r}
prop_city %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimates and CIs for Each City",
    x = "City, State",
    y = "Estimates",
    caption = "Data from the Washington Post") +
   theme(plot.title = element_text(hjust = 0.5))
  
```


## Problem 3

#### Generate the simulation

```{r}
sim_func = function(n = 30, mu, sigma = 5){
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  t_test = t.test(x, conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
  t_test
}
```

#### Set $\mu$ = 0

```{r, cache=TRUE}
output = vector("list", length = 5000)

for (i in 1:5000){
  
  output[[i]] = sim_func(mu = 0)
  
}

bind_rows(output) %>% 
  unnest()
```

#### Repeat above for $\mu$ = 1,2,3,4,5,6

```{r, cache=TRUE}
sim_results =
  tibble(true_mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output = map(.x = true_mu, ~rerun(5000, sim_func(mu = .x))),
    test_estimate = map(output, bind_rows)
    ) %>% 
  select(-output) %>% 
  unnest()

sim_results
  
```

#### Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. 

```{r}
sim_results %>% 
  group_by(true_mu) %>% 
  summarise(total = n(),
            rejected = sum(p.value < 0.05)) %>% 
  mutate(proportion = rejected/total) %>% 
  ggplot(aes(y = proportion, x = true_mu)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "True μ ",
    y = "Power of the test",
    title = "Power of the Test over different True μ"
    ) +
   theme(plot.title = element_text(hjust = 0.5))
  
```

As μ increases, the proportion of null which was rejected increases as well, which means that as effect size increases, power of the test increases.


#### Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. Overlay a second plot (in BLUE) of the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

```{r}
rejected_muhat = 
  sim_results %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mu) %>% 
  summarise(ave_muhat = mean(estimate))

rejected_muhat
```

```{r}
sim_results %>% 
  group_by(true_mu) %>% 
  summarise(ave_muhat = mean(estimate)) %>% 
  ggplot(aes(x = true_mu, y = ave_muhat)) +
  geom_point(color = "red") +
  geom_smooth(se = FALSE , color = "red") +
  geom_point(data = rejected_muhat,
            color = "blue") + 
  geom_smooth(data = rejected_muhat, se = FALSE) +
  labs(
    x = "True μ",
    y = "Average Estimate of μ^",
    title = " Average Estimate of μ^ of All Sample and Null Rejected-sample"
    ) +
  theme(plot.title = element_text(hjust = 0.5))
  
```

The average estimate of μ^ is approximately equal to the true $\mu$ for all samples. However, in the rejected sample, the average estimate of μ^ is approximately equal to true $\mu$ only when true $\mu$ is greater than 4. This means the average estimate is a good approximation of the true $\mu$ when effect size is large, while it is not a good approximation when effect size is small.


